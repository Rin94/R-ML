as.character(sms_corpus[[1]])
## visualizar más de dos documentos usar la funcion lappy
lapply(sms_corpus[1:2], as.character)
## Limpiar el corpus, transformar los datos a minusculas
sms_corpus_clean<-tm_map(sms_corpus, content_transformer(tolower))
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])
## remover numeros
sms_corpus_clean<-tm_map(sms_corpus_clean,removeNumbers)
## Our next task is to remove filler words such: to, and, but, or from our
#SMS message. These terms are known as STOP WORDS, cuz do not provide much
#useful inofrmation and last remove these words
sms_corpus_clean<-tm_map(sms_corpus_clean,removeWords,stopwords())
## Next remove any punctiation from the text messages
sms_corpus_clean<-tm_map(sms_corpus_clean,removePunctuation)
removePunctuation("hello...,...world")
## Reducing words to ther root by the process called STEMMING
#install.packages("SnowballC")
library(SnowballC)
wordStem(c("learn", "learned", "learning","learns"))
wordStem(c("videogame","game","gaming","gamer"))
sms_corpus_clean<-tm_map(sms_corpus_clean, stemDocument)
## REMOVE white_spaces
sms_corpus_clean<-tm_map(sms_corpus_clean, stripWhitespace)
print(as.character(sms_corpus[1:3]))
as.character(sms_corpus[1:3])
print(as.character(sms_corpus_clean[1:3]))
as.character(sms_corpus_clean[1:3])
### Data preparation
##Now that the data are processed to our liking, the final step is to split
#the messages into invidual components through a process called tokenization
#Tokenizer
#rows: documents, columns:words
#hace una columna de cuantas veces se muestra una palabra
sms_dtm<-DocumentTermMatrix(sms_corpus_clean)
sms_dtm2<-DocumentTermMatrix(sms_corpus,
control = list(tolower=TRUE,
removeNumbers=TRUE,
stopwords=TRUE,
removePunctuation=TRUE,
stemming=TRUE))
sms_dtm
sms_dtm2
##the order of preparation matters
### Data preparation- creating training and test datasets
sms_dtm_train<-sms_dtm[1:4169,]
sms_dtm_test<-sms_dtm[4170:5574,]
sms_train_labels<-sms_raw[1:4169,]$type
sms_test_labels<-sms_raw[4170:5574,]$type
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
### Visualizing text data - WORD CLOUDS
#install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus_clean,min.freq = 50, random.order = FALSE)
options(warnings=2)
spam<-subset(sms_raw,type=="spam")
ham<-subset(sms_raw,type=="ham")
wordcloud(spam$text,max.words = 40,scale=c(3,0.5))
wordcloud(ham$text,max.words = 40, scale=c(3,0.5))
### Data preparation
## para hacerlo más limpio vamos a reducir el numero de caracteristicas,
##quitando palabras que tengan menos de 5 menciones usando
#FINDFREQTERMS
findFreqTerms(sms_dtm_train,5)
sms_freq_words<-findFreqTerms(sms_dtm_train,5)
str(sms_freq_words)
sms_dtm_freq_train<-sms_dtm_train[,sms_freq_words]
sms_dtm_freq_test<-sms_dtm_test[,sms_freq_words]
##hacer una funcion que nos diga si es spam o no
convert_counts<-function(x){
x<-ifelse(x>0,"Yes","No")
}
sms_train<-apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test<-apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
install.packages("e1071")
library(e1071)
#building the classifier
sms_classifier<-naiveBayes(sms_train,sms_train_labels)
#making predictions
sms_tes_pred<-predict(sms_classifier,sms_test)
#example
library(gmodels)
CrossTable(sms_tes_pred,sms_test_labels,prop.chisq = FALSE, prop.t = FALSE,
dnn = c("predicted","actual"))
## Mejorando con el laplace
sms_classifier2<-naiveBayes(sms_train,sms_train_labels,laplace = 1)
sms_tes_pred2<-predict(sms_classifier2,sms_test)
library(gmodels)
CrossTable(sms_tes_pred2,sms_test_labels,prop.chisq = FALSE, prop.t = FALSE,
dnn = c("predicted","actual"))
install.packages("e1071")
install.packages("e1071")
install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
#making predictions
sms_test_pred <- predict(sms_classifier, sms_test)
#making predictions
sms_test_pred <- predict(sms_classifier, sms_test)
head(sms_test_pred)
install.packages("gmodels")
install.packages("gmodels")
library(gmodels)
length(sms_test_pred)
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
sms_classifier2<-naiveBayes(sms_train,sms_train_labels,laplace = 1)
sms_tes_pred2<-predict(sms_classifier2,sms_test)
CrossTable(sms_tes_pred2,sms_test_labels,prop.chisq = FALSE, prop.t = FALSE,
dnn = c("predicted","actual"))
CrossTable(sms_tes_pred2,sms_test_labels,prop.chisq = FALSE, prop.t = FALSE,
dnn = c("predicted","actual"))
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
length(sms_test_pred)
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
sms_classifier2<-naiveBayes(sms_train,sms_train_labels,laplace = 1)
sms_classifier2<-naiveBayes(sms_train,sms_train_labels,laplace = 1)
sms_tes_pred2<-predict(sms_classifier2,sms_test)
CrossTable(sms_tes_pred2,sms_test_labels,prop.chisq = FALSE, prop.t = FALSE,
dnn = c("predicted","actual"))
install.packages(e1071)
install.packages("e1071")
install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
#making predictions
sms_test_pred <- predict(sms_classifier, sms_test)
head(sms_test_pred)
length(sms_test_pred)
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
table(sms_tes_pred,sms_test_labels)
table(sms_classifier2,sms_tes_pred2)
sms_classifier2<-naiveBayes(sms_train,sms_train_labels,laplace = 1)
sms_tes_pred2<-predict(sms_classifier2,sms_test)
sms_tes_pred2<-predict(sms_classifier2,sms_test)
table(sms_classifier2,sms_tes_pred2)
table(sms_tes_pred2,sms_test_labels)
length(sms_test_pred)
table(sms_test_pred,sms_test_labels)
length(sms_test_pred)
table(sms_test_pred,sms_test_labels)
sms_classifier2<-naiveBayes(sms_train,sms_train_labels,laplace = 1)
sms_tes_pred2<-predict(sms_classifier2,sms_test)
table(sms_tes_pred2,sms_test_labels)
credit<-read.csv("data/credit.csv")
table(credit$checking_balance)
table(credit$savings_balance)
## check for distribution of some values
summary(credit$months_loan_duration)
summary(credit$amount)
table(credit$checking_balance)
table(credit$savings_balance)
## check for distribution of some values
summary(credit$months_loan_duration)
summary(credit$amount)
str(credit)
str(credit)
table(credit$checking_balance)
table(credit$savings_balance)
## check for distribution of some values
summary(credit$months_loan_duration)
summary(credit$amount)
credit$default
credit$default <- factor(credit$default,
levels=c(1,2),
labels=c("No","Si"))
table(credit$default)
set.seed(123)
train_sample<-sample(1000,900)
str(train_sample)
credit_train<-credit[train_sample,]
credit_test<-credit[-train_sample,]
# verify the dataset of train
prop.table(table(credit_train$default))
#veryfi the dataset of test
prop.table(table(credit_test$default))
library(C50)
credit_model<-C5.0(credit_train[-17],credit_train$default,type="class")
credit_model ##gives info of the decision tree
summary(credit_model)
library(gmodels)
credit_pred<-predict(credit_model,credit_test)
CrossTable(credit_test$default,credit_pred, prop.chisq = FALSE,
prop.c = FALSE, prop.r=FALSE, dnn=c("actual default", "predict default"))
model_2<-C5.0(credit_train[-17],credit_train$default,type="Class",trials = 10)
model_2
summary(model_2)
credit_boost<-predict(model_2,credit_test)
table(credit_boost)
CrossTable(credit_test$default, credit_boost, prop.chisq = FALSE, prop.c = FALSE,
prop.r = FALSE,
dnn=c("actual defaul", "predicted default"))
matrix_dimensions<-list(c("no","yes"), c("no","yes"))
names(matrix_dimensions)<-c("predicted","actual")
matrix_dimensions
error_cost<-matrix(c(0,1,4,0),nrow = 2, dimnames = matrix_dimensions)
error_cost
credit_cost<-C5.0(credit_train[-17],credit_train$default,type="Class",costs=error_cost)
credit_cost_pred<-predict(credit_cost,credit_test)
matrix_dimensions<-list(c("no","yes"), c("no","yes"))
names(matrix_dimensions)<-c("predicted","actual")
matrix_dimensions
error_cost<-matrix(c(0,1,4,0),nrow = 2, dimnames = matrix_dimensions)
error_cost
credit_cost<-C5.0(credit_train[-17],credit_train$default,type="Class",costs=error_cost)
credit_cost_pred<-predict(credit_cost,credit_test)
matrix_dimensions<-list(c("no","yes"), c("no","yes"))
names(matrix_dimensions)<-c("predicted","actual")
matrix_dimensions
error_cost<-matrix(c(0,1),nrow = 2, dimnames = matrix_dimensions)
matrix_dimensions<-list(c("no","yes"), c("no","yes"))
names(matrix_dimensions)<-c("predicted","actual")
matrix_dimensions
error_cost<-matrix(c(1,1,1,1),nrow = 2, dimnames = matrix_dimensions)
error_cost
credit_cost<-C5.0(credit_train[-17],credit_train$default,type="Class",costs=error_cost)
credit_cost_pred<-predict(credit_cost,credit_test)
matrix_dimensions <- list(c("no", "yes"), c("no", "yes"))
names(matrix_dimensions) <- c("predicted", "actual")
matrix_dimensions
error_cost <- matrix(c(0, 1, 1, 0), nrow = 2,
dimnames = matrix_dimensions)
error_cost
credit_cost <- C5.0(credit_train[-17], credit_train$default, trials = 8,
costs =error_cost)
credit_cost_pred<-predict(credit_cost,credit_test)
CrossTable(credit_test$default, credit_cost_pred, prop.chisq = FALSE, prop.c = FALSE,
prop.r = FALSE,
dnn=c("actual defaul", "predicted default"))
hongos<-read.csv("data/mushrooms.csv",stringsAsFactors = TRUE)
str(hongos)## describe the types of data
hongos$veil_type<-NULL#By assigning null a vector, R eliminates the feature from the mushrooms dataframe
table(hongos$type)#Ver los tipos de hongos
summary(hongos)
tail(hongos)
set.seed(123)
train_sample<-sample(8124,5686)
str(train_sample)
hongos_train<-hongos[train_sample,]
hongos_test<-hongos[-train_sample,]
# verify the dataset of train
prop.table(table(hongos_train$type))
#veryfi the dataset of test
prop.table(table(hongos_test$type))
library(C50)
modelo_hongos<-C5.0(hongos_train[-1],hongos_train$type,type="class")
modelo_hongos
summary(modelo_hongos)
library(gmodels)
hongos_pred<-predict(modelo_hongos,hongos_test)
CrossTable(hongos_test$type,hongos_pred, prop.chisq = FALSE,
prop.c = FALSE, prop.r=FALSE, dnn=c("actual default", "predict default"))
launch<-read.csv("Data/challenger.csv")
str(launch)
summary(launch)
lengths(launch)
summary(launch)
lengths(launch)
b<-cov(launch$temperature, launch$distress_ct)/var(launch$temperature)
b
a<-mean(launch$distress_ct)-b*mean(launch$temperature)
a
c_orr<-cov(launch$temperature, launch$distress_ct)/(sd(launch$temperature)*sd(launch$distress_ct))
c_orr
cor(launch$temperature,launch$distress_ct)
reg<-function(y,x){
x<-as.matrix(x)
x<-cbind(Intercept=1,x)#agregamos columna a x y la llenamos con unos
b<-solve(t(x)%*%x)%*%t(x)%*%y
colnames(b)<-"estimate"
print(b)
}
str(launch)
reg(y=launch$distress_ct, x=launch[3])
#x<-claunch[-2]
x1<-c(launch[1])
x2<-c(launch[3])
x3<-c(launch[4])
x4<-c(launch[5])
rocket_ln<-lm(launch$distress_ct~launch$temperature,data = launch)
rocket_ln
rocket_ln<-lm(launch$distress_ct~.,data = launch)
rocket_ln
summary(rocket_ln)
launch_1<-launch[-2]
launch_1<-launch_1[-1]
launch_1
reg(launch$distress_ct,launch_1[1:3])
insurance<-read.csv("Data/insurance.csv",stringsAsFactors = TRUE)
str(insurance)
summary(insurance)
summary(insurance$charges)#charges equals expenses
hist(insurance$charges)
colnames(insurance)
cor(insurance[c("age","bmi","children","charges")])
pairs(insurance[c("age","bmi","children","charges")])
#aumented our dataset
install.packages("psych")
library(psych)
pairs.panels(insurance[c("age","bmi","children","charges")])
ins_model<-lm(charges~age+children+bmi+sex+smoker+region,data=insurance)
ins_model
summary(ins_model)
insurance$age2<-insurance$age^2
insurance$bmi30<-ifelse(insurance$bmi>=30,1,0)
ins_model2<-lm(charges~age+age2+children+bmi+sex+bmi30*smoker+region,data=insurance)
summary(ins_model2)
wine<-read.csv("Data/winequality-white.csv",stringsAsFactors = TRUE)
str(wine)
summary(wine)
sum(is.na(wine))
## correlation
library(psych)
pairs.panels(wine[colnames(wine)])
cor(wine)
hist(wine$quality)
set.seed(2)
train_sample<-sample(4898,3750)
train_sample
wine_train<-wine[train_sample,]
wine_test<-wine[-train_sample,]
str(wine_test)
str(wine_train)
library(rpart)
## m<-rpart(dv~iv, data=mydata)
#p<-predict(m,test,type="vector")
###vector->numeric values
###class->classification classes
###prob->predicted class probabilites
winemodel<-rpart(quality~.,data=wine_train)
winemodel
###Visualizing decision trees
library(rpart.plot)
rpart.plot(winemodel,digits = 3)
#extra parameters
rpart.plot(winemodel,digits=4,fallen.leaves = TRUE,type=3,extra = 101)
###Evaluating model performance
p.rpart<-predict(winemodel,wine_test)
summary(p.rpart)
summary(wine_test$quality)
cor(p.rpart,wine_test$quality)
#MAE Mean Absolute Error
MAE<-function(actual,predicted){
mean(abs(actual-predicted))
}
MAE(p.rpart,wine_test$quality)
mean(wine_train$quality)
MAE(5.87,wine_test$quality)
summary(p.rpart)
summary(wine_test$quality)
cor(p.rpart,wine_test$quality)
#MAE Mean Absolute Error
MAE<-function(actual,predicted){
mean(abs(actual-predicted))
}
print("Error de predicción del modelo")
MAE(p.rpart,wine_test$quality)
print("Media de la distribución de la calidad del vino")
mean(wine_train$quality)
print("La relación entre el error y la media de la calidad del vino")
MAE(5.87,wine_test$quality)
concrete<-read.csv("Data/concrete.csv")
str(concrete)
normalize<-function(x){
return ((x-min(x))/ (max(x)- min(x)))
}
concrete_norm<-as.data.frame(lapply(concrete, normalize))
#lappy nos evita usar ciclo, donde le decimos que todas las columnas, le aplica la funci??n normalize
#Comprobamos si estan normalizados
summary(concrete_norm $ strength)
#Comprobamos con los valores normales
summary(concrete $ strength)
normalize<-function(x){
return ((x-min(x))/ (max(x)- min(x)))
}
concrete_norm<-as.data.frame(lapply(concrete, normalize))
#lappy nos evita usar ciclo, donde le decimos que todas las columnas, le aplica la funci??n normalize
#Comprobamos si estan normalizados
print("datos normalizados")
summary(concrete_norm $ strength)
#Comprobamos con los valores normales
print("datos sin normalizar")
summary(concrete $ strength)
concrete_train<-concrete_norm[1:773,]
concrete_test<-concrete_norm[774:1030,]
library(neuralnet)
set.seed(12354)
#m<- neuralnet(target-predictors, data= mydata, hidden=1)
concrete_model<-neuralnet(strength ~ cement +slag + ash + water + superplastic + coarseagg + fineagg +age, data=concrete_train)
plot(concrete_model, rep="best")
#Evaluar el desempenio del modelo
#Pasarle nuestro modelo, con muestros datos de prueba, la parte es la comlumna, no se pasa la columna 9
model_result<-compute(concrete_model, concrete_test[1:8])
#vamos a guardar los resultados de nuestra prediccion
predicted_strength<-model_result$net.result
cor(predicted_strength,concrete_test$strength)[,1]
concrete_model2<-neuralnet(strength ~ cement +slag + ash + water + superplastic + coarseagg + fineagg +age, data=concrete_train,hidden=5)
plot(concrete_model2, rep="best")
model_result2<-compute(concrete_model2, concrete_test[1:8])
predicted_strength2<-model_result2$net.result
cor(predicted_strength2,concrete_test$strength)
letters<-read.csv("Data/letterdata.csv")
str(letters)
letters_train<-letters[1:16000,]
letters_test<-letters[16001:20000,]
table(letters$letter)
letters_train<-letters[1:16000,]
letters_test<-letters[16001:20000,]
library(e1071)
library(kernlab)
## suport vector m<-ksvm(target~predictors,data=mydata,kernel="rbfdot",c=1)
letter_model<-ksvm(letter~.,data=letters_train, kernel="vanilladot")
letter_model
letter_predictions<-predict(letter_model,letters_test)
letter_predictions <- predict(letter_model, letters_test)
head(letter_predictions)
#check other letters
table(letter_predictions, letters_test$letter)
agreement <- letter_predictions == letters_test$letter
table(agreement)
#accuracy table
prop.table(table(agreement))
letter_classifier_rbf <- ksvm(letter ~ ., data = letters_train,
kernel = "rbfdot")
letter_predictions<-predict(letter_classifier_rbf,letters_test)
table(letter_predictions, letters_test$letter)
agreement <- letter_predictions == letters_test$letter
table(agreement)
#Market Basket Analysis- tecnica recomendada para citas, dating sites, finding dangeous among medications
#9832 TRANSACTIONES (30 TRANSACTION PER HOUR), SE REMOVI?? EL VALOR DE MARCA.
#M??s granularidad,m??s datos
#cada transacci??n cada liea
#Matriz esparcida la mayoria de las transacciones tiene 0
library("arules")
groceries <- read.transactions("groceries.csv",sep=",")#entienda que es una base de datos transaccional, separacion por comas
#Market Basket Analysis- tecnica recomendada para citas, dating sites, finding dangeous among medications
#9832 TRANSACTIONES (30 TRANSACTION PER HOUR), SE REMOVI?? EL VALOR DE MARCA.
#M??s granularidad,m??s datos
#cada transacci??n cada liea
#Matriz esparcida la mayoria de las transacciones tiene 0
library("arules")
groceries <- read.transactions("Data/groceries.csv",sep=",")#entienda que es una base de datos transaccional, separacion por comas
#densidad cuantos unos hay
summary(groceries)
inspect(groceries[1:5])
itemFrequency(groceries[,1:3])
itemFrequencyPlot(groceries, support=0.2)#aparezca el 10% de las observaciones
itemFrequencyPlot(groceries, topN=20)
image(groceries[1:5])#x articulos y transacciones
image(sample(groceries, 100))
apriori(groceries)
groceryrules<-apriori(groceries, parameter = list(support=0.006, confidence=0.25, minlen=2))
groceryrules
summary(groceryrules)
inspect(groceryrules[1:50])#mostrar las reglas del 1 a la especificada
#se debe de buscar mucho lift para ver que no sea coincdencia
inspect(sort(groceryrules,by="lift")[1:10])#ordenar por lift
berryrules<-subset(groceryrules, items %in% "berries")
inspect(berryrules)
milkrules<-subset(groceryrules, items %in%c ("whole milk",'berries'))
inspect(sort(milkrules,by="lift")[1:30])
write(groceryrules, file = "groceryrules.csv",sep=",", quote=TRUE, row.names=FALSE)
sns<-read.csv("Data/snsdata.csv")
str(sns)
table(sns$gender,useNA = "ifany")
summary(sns$age)#max age and min age are unreasonable for teenagers
summary(sns$age)#max age and min age are unreasonable for teenagers
sns$female<-ifelse(sns$gender=="F" & !is.na(sns$gender),1,0)
sns$nogender<-ifelse(is.na(sns$gender),1,0)
table(sns$female)
table(sns$nogender)
###Data Preparation - Inputing missing values
sns$age<-ifelse(sns$age>=13&sns$age<20,sns$age,NA)
mean(sns$age, na.rm=TRUE)
#calcula la media de la edad por los valores de año
aggregate(data=sns,age~gradyear,mean,na.rm=TRUE)
#ave() regresa un vector con el grupo de medias repetidas, asi que el resultado
#es igual al vector original
ave_age<-ave(sns$age, sns$gradyear,FUN=function(x) mean(x,na.rm=TRUE))
ave_age
sns$age<-ifelse(is.na(sns$age),ave_age,sns$age)
summary(sns$age)
library(stats)
interests<-sns[5:40]
interests
#normalize the interest-lapply returns matrix, as.data.frame convert to dataframe
interests_xyz<-as.data.frame(lapply(interests,scale))
interests_xyz
set.seed(2345)
sns_clusters<-kmeans(interests_xyz,5)
sns_clusters$size
sns_clusters$centers
sns_clusters$cluster
sns$cluster_id<-sns_clusters$cluster
sns_clusters$size
sns_clusters$centers
sns_clusters$cluster
sns$cluster_id<-sns_clusters$cluster
sns$cluster_id<-sns_clusters$cluster
###improving model performance
sns[1:5,c("cluster_id","gender","age","friends")]
aggregate(data=sns,age~cluster_id,mean)
aggregate(data=sns,female~cluster_id,mean)
aggregate(data=sns,friends~cluster_id,mean)
